import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import shutil
import re

def organize_existing_files():
    """ê¸°ì¡´ íŒŒì¼ë“¤ì„ ë‚ ì§œë³„ í´ë”ë¡œ ì •ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    base_folder = "leaflets"
    
    if not os.path.exists(base_folder):
        return
    
    print("ğŸ“ ê¸°ì¡´ íŒŒì¼ë“¤ì„ ë‚ ì§œë³„ í´ë”ë¡œ ì •ë¦¬ ì¤‘...")
    
    # leaflets í´ë” ë‚´ì˜ ëª¨ë“  íŒŒì¼ ê²€ìƒ‰
    for filename in os.listdir(base_folder):
        if filename.endswith('.jpg') and filename.startswith('emart_leaflet_'):
            # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ì˜ˆ: emart_leaflet_20250724_01.jpg)
            date_match = re.search(r'emart_leaflet_(\d{8})_\d{2}\.jpg', filename)
            if date_match:
                date_str = date_match.group(1)  # 20250724
                # ë‚ ì§œ í˜•ì‹ ë³€í™˜ (20250724 -> 2025-07-24)
                formatted_date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
                
                # ë‚ ì§œë³„ í´ë” ìƒì„±
                date_folder = os.path.join(base_folder, formatted_date)
                os.makedirs(date_folder, exist_ok=True)
                
                # íŒŒì¼ ì´ë™
                old_path = os.path.join(base_folder, filename)
                # ìƒˆ íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ë¶€ë¶„ ì œê±° (emart_leaflet_20250724_01.jpg -> emart_leaflet_01.jpg)
                new_filename = re.sub(r'emart_leaflet_\d{8}_', 'emart_leaflet_', filename)
                new_path = os.path.join(date_folder, new_filename)
                
                if os.path.exists(old_path) and not os.path.exists(new_path):
                    shutil.move(old_path, new_path)
                    print(f"âœ… {filename} -> {formatted_date}/{new_filename}")

def scrape_emart_leaflets():
    # ê¸°ë³¸ ì €ì¥ í´ë” ìƒì„±
    base_folder = "leaflets"
    os.makedirs(base_folder, exist_ok=True)
    
    # ì˜¤ëŠ˜ ë‚ ì§œë¡œ í•˜ìœ„ í´ë” ìƒì„±
    today = datetime.now().strftime("%Y-%m-%d")
    today_folder = os.path.join(base_folder, today)
    os.makedirs(today_folder, exist_ok=True)
    
    url = "https://emartapp.emart.com/leaflet/leafletView_EL.do?&image_pop=true"
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.2 Safari/605.1.15",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1"
    }
    
    try:
        print("ğŸ” ì´ë§ˆíŠ¸ ì „ë‹¨ì§€ í˜ì´ì§€ì— ì ‘ì† ì¤‘...")
        res = requests.get(url, headers=headers, timeout=30)
        res.encoding = 'utf-8'
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # HTML ì €ì¥ (ë””ë²„ê¹…ìš©)
        with open("emart.html", "w", encoding="utf-8") as f:
            f.write(res.text)
        print("ğŸ“„ HTML í˜ì´ì§€ ì €ì¥ ì™„ë£Œ (emart.html)")
        
        # data-src ì†ì„±ì„ ê°€ì§„ ì´ë¯¸ì§€ íƒœê·¸ ì°¾ê¸°
        img_tags = soup.select("div.img_detail img[data-src]")
        
        if not img_tags:
            print("âŒ ì „ë‹¨ì§€ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"âœ… {len(img_tags)}ê°œì˜ ì „ë‹¨ì§€ ì´ë¯¸ì§€ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
        
        success_count = 0
        for i, img in enumerate(img_tags, 1):
            img_url = img.get("data-src")
            alt_text = img.get("alt", f"ì „ë‹¨ì§€ {i}ë©´")
            
            if not img_url:
                print(f"âš ï¸ {i}ë²ˆì§¸ ì´ë¯¸ì§€ì˜ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if img_url.startswith("/"):
                img_url = "https://stimg.emart.com" + img_url
            
            try:
                print(f"ğŸ“¥ {i}ë²ˆì§¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì¤‘... ({alt_text})")
                img_response = requests.get(img_url, headers=headers, timeout=30)
                img_response.raise_for_status()
                
                # íŒŒì¼ëª… ìƒì„± (ë‚ ì§œë³„ í´ë” êµ¬ì¡°)
                filename = os.path.join(today_folder, f"emart_leaflet_{i:02d}.jpg")
                
                with open(filename, "wb") as f:
                    f.write(img_response.content)
                
                file_size = len(img_response.content) / 1024  # KB
                print(f"âœ… {filename} ì €ì¥ ì™„ë£Œ ({file_size:.1f}KB)")
                success_count += 1
                
                # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
                time.sleep(1)
                
            except requests.exceptions.RequestException as e:
                print(f"âŒ {i}ë²ˆì§¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            except Exception as e:
                print(f"âŒ {i}ë²ˆì§¸ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        print(f"\nğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {success_count}ê°œì˜ ì´ë¯¸ì§€ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: ./{today_folder}/")
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ì›¹í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {e}")
    except Exception as e:
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    # ê¸°ì¡´ íŒŒì¼ë“¤ì„ ë‚ ì§œë³„ í´ë”ë¡œ ì •ë¦¬
    organize_existing_files()
    
    # ìƒˆë¡œìš´ ì „ë‹¨ì§€ í¬ë¡¤ë§ ì‹¤í–‰
    scrape_emart_leaflets()
